# Neural Network Models for Cross-Language Code Synthesis and Translation

## Authors

- **Amrutha Muralidhar**  
- **Ananya Aithal**  
- **G Sanjana Hebbar**  
- **Dr. Kavitha Sooda**  

*Department of Computer Science and Engineering*  
*B.M.S. College of Engineering, Bangalore, India*  

---

## Abstract

This project evaluates the performance of three state-of-the-art neural network models—**TransCoder**, **CodeT5**, and **CodeBERT**—for cross-language code synthesis and translation. Using the **CodeXGlue** dataset, we assess these models based on two key metrics:

1. **Code Similarity Score (CSS)**
2. **Overall Execution Score (OES)**

**Key Findings:**
- CodeT5 achieves the highest translation accuracy
- TransCoder struggles with semantic errors
- CodeBERT performs reasonably well but faces challenges with:
  - Complex control flow
  - Abstract constructs

These insights provide valuable guidance for developing improved code translation models, with applications in:
- Software engineering
- Programming education

---

## Introduction

Cross-language code translation is a critical task in software engineering, enabling developers to:
- Port code between programming languages efficiently
- Reduce manual translation errors
- Accelerate development cycles

**Recent Advances:**  
Neural network models have shown promise in automating this process.

**This Study Evaluates:**
1. TransCoder
2. CodeT5  
3. CodeBERT  

**Methodology:**
- Performance analysis on **CodeXGlue** dataset
- Identification of strengths and weaknesses
- Actionable insights for model improvement

**Research Significance:**  
Advances the field of automated code translation through empirical evaluation of cutting-edge models.

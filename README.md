Neural Network Models for Cross-Language Code Synthesis and Translation
Authors
Amrutha Muralidhar

Ananya Aithal

G Sanjana Hebbar

Dr. Kavitha Sooda

Department of Computer Science and Engineering
B.M.S. College of Engineering, Bangalore, India

Abstract
This project evaluates the performance of three state-of-the-art neural network models—TransCoder, CodeT5, and CodeBERT—for cross-language code synthesis and translation. Using the CodeXGlue dataset, we assess these models based on two key metrics: Code Similarity Score (CSS) and Overall Execution Score (OES). Our findings reveal that CodeT5 achieves the highest translation accuracy, while TransCoder struggles with semantic errors. CodeBERT performs reasonably well but faces challenges in translating complex control flow and abstract constructs. These insights provide valuable guidance for the development of improved code translation models, with applications in software engineering and programming education.

Introduction
Cross-language code translation is a critical task in software engineering, enabling developers to port code between programming languages efficiently. Recent advances in neural network models have shown promise in automating this process. This study evaluates three prominent models—TransCoder, CodeT5, and CodeBERT—to identify their strengths and weaknesses in code translation tasks. By analyzing their performance on the CodeXGlue dataset, we aim to provide actionable insights for improving these models and advancing the field of automated code translation.
